Sys.setenv(NOAWT=TRUE)#
library(tm)#
library(gplots)#
library(austin)#
library(RWekajars)#
set.seed(100)
help(austin)
Sys.setenv(NOAWT=TRUE)#
library(tm)#
library(gplots)#
library(austin)#
library(RWekajars)#
set.seed(100)
files.path <- '/ALEX/poslania/analysis/georgia/texts_ge'#
graphs.path <- '/ALEX/poslania/analysis/georgia/graphs'#
data.output <- '/ALEX/poslania/analysis/georgia/data'
--------------------------------------------------------------------------------#
# BODY#
# --------------------------------------------------------------------------------#
# generate text corpus#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="georgian", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
# cleaning texts  #
text.corpus.format<-textcorpus#
	text.corpus.format <- tm_map(text.corpus.format, tolower)#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
	# strip whitespace#
	text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)#
    text.corpus.format  <- tm_map(text.corpus.format, removeWords)#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
presname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  presname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}
estimate policy positions#
ref.left <- seq(1,N,1)[presname=="Shevardnadze" & year=="1997"]#
ref.right <- seq(1,N,1)[presname=="Saakashvili" & year=="2012"]
Load wordfish function#
source('/ALEX/poslania/analysis/wordfish_function.R')#
#
results <- wordfish(input=wcdata, dir=c(ref.left,ref.right), boot=F, writeout=T)#
#
results <- wordfish(input=wcdata, dir=c(ref.left,ref.right), boot=T, nsim=250, writeout=T)#
#
positions <- results$documents[,1]#
words <- results$words[,1]#
results$documents
ref.left <- seq(1,N,1)[presname=="Shevardnadze" & year=="1997"]#
ref.right <- seq(1,N,1)[presname=="Saakashvili" & year=="2005"]
source('/ALEX/poslania/analysis/wordfish_function.R')#
#
results <- wordfish(input=wcdata, dir=c(ref.left,ref.right), boot=F, writeout=T)
positions <- results$documents[,1]#
words <- results$words[,1]#
results$documents
y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(year)-N)#
#
# label names#
lab.name <- paste(presname,year)#
#
out.file <- paste(graphs.path,"estimated_positions_georgia_ge.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "Georgia"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=1#
		, pch = 16#
		, col = "orange"#
		, xlim = c(-2, 2.4),#
		ylim=c(0,9),#
		)#
text(x=positions,ydummy, labels=lab.name, cex=0.9, pos=3)#
abline(v=0,col=3,lty=5)#
dev.off()
Sys.setenv(NOAWT=TRUE)#
library(tm)#
library(gplots)#
library(austin)#
library(RWekajars)#
set.seed(100)
files.path <- '/ALEX/poslania/analysis/georgia/texts_ge'#
graphs.path <- '/ALEX/poslania/analysis/georgia/graphs'#
data.output <- '/ALEX/poslania/analysis/georgia/data'#
# --------------------------------------------------------------------------------#
# BODY#
# --------------------------------------------------------------------------------#
# generate text corpus#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="georgian", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
# cleaning texts  #
text.corpus.format<-textcorpus#
	text.corpus.format <- tm_map(text.corpus.format, tolower)#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
	# strip whitespace#
	text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)#
    text.corpus.format  <- tm_map(text.corpus.format, removeWords)#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
presname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  presname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}
ref.left <- seq(1,N,1)[presname=="Shevardnadze" & year=="1997"]#
ref.right <- seq(1,N,1)[presname=="Saakashvili" & year=="2005"]
source('/ALEX/poslania/analysis/wordfish_function.R')#
#
results <- wordfish(input=wcdata, dir=c(ref.left,ref.right), boot=F, writeout=T)
positions <- results$documents[,1]#
words <- results$words[,1]#
results$documents
y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(year)-N)#
#
# label names#
lab.name <- paste(presname,year)#
#
out.file <- paste(graphs.path,"estimated_positions_georgia_ge.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "Georgia"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=1#
		, pch = 16#
		, col = "orange"#
		, xlim = c(-2, 2.4),#
		ylim=c(0,7),#
		)#
text(x=positions,ydummy, labels=lab.name, cex=0.9, pos=3)#
abline(v=0,col=3,lty=5)#
dev.off()
y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(year)-N)#
#
# label names#
lab.name <- paste(presname,year)#
#
out.file <- paste(graphs.path,"estimated_positions_georgia_ge.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "Georgia"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=1#
		, pch = 16#
		, col = "orange"#
		, xlim = c(-2, 2.4),#
		ylim=c(0,9),#
		)#
text(x=positions,ydummy, labels=lab.name, cex=0.9, pos=3)#
abline(v=0,col=3,lty=5)#
dev.off()
y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(year)-N)#
#
# label names#
lab.name <- paste(presname,year)#
#
out.file <- paste(graphs.path,"estimated_positions_georgia_ge.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "Georgia"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=1#
		, pch = 16#
		, col = "orange"#
		, xlim = c(-2.2, 2.4),#
		ylim=c(0,10),#
		)#
text(x=positions,ydummy, labels=lab.name, cex=0.9, pos=3)#
abline(v=0,col=3,lty=5)#
dev.off()
y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(year)-N)#
#
# label names#
lab.name <- paste(presname,year)#
#
out.file <- paste(graphs.path,"estimated_positions_georgia_ge.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "Georgia"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=1#
		, pch = 16#
		, col = "orange"#
		, xlim = c(-2.2, 2.4),#
		ylim=c(0,8),#
		)#
text(x=positions,ydummy, labels=lab.name, cex=0.9, pos=3)#
abline(v=0,col=3,lty=5)#
dev.off()
library(foreign)#
ter <- stata.data <- read.dta("cl_terms.dta")#
attach(ter)
library(deldir)   # Note: need to install from CRAN if not already#
library(Hmisc)#
library(foreign)
install.packages(deldir)
Sys.setenv(http_proxy="http://staff-proxy.dcu.ie:8080")
Sys.getenv("http_proxy")
Sys.setenv(http_proxy="http://proxy.dcu.ie:8080")
Sys.getenv("http_proxy")
Sys.setenv(http_proxy="http://proxy.dcu.ie:8080")
Sys.getenv("http_proxy")
Sys.setenv(http_proxy="http://proxy.dcu.ie:8080")
Sys.setenv("http_proxy"="http://proxy.dcu.ie:8080")
Sys.setenv(HTTP_PROXY="http://proxy.dcu.ie:8080")
Sys.setenv("HTTP_PROXY"="http://proxy.dcu.ie:8080")
install.packages(deldir)
install.packages("deldir")
Sys.setenv("http_proxy="http://user:password@proxy.dcu.ie:8080/" ")
Sys.setenv("http_proxy="http://user:password@proxy.dcu.ie:8080/")
Sys.setenv(http_proxy="http://user:password@proxy.dcu.ie:8080/")
Sys.setenv(http_proxy=”http://baturoa:myadel05@proxy.dcu.ie:8080″)
Sys.setenv(http_proxy=http://proxy.dcu.ie:8080/ http_proxy_user=ask)
Sys.setenv(http_proxy=http://proxy.dcu.ie:8080/http_proxy_user=ask)
Sys.setenv(http_proxy="http://baturoa:myadel05@proxy.dcu.ie:8080″)
Sys.setenv(http_proxy="http://baturoa:myadel05@proxy.dcu.ie:8080")
Sys.setenv(http_proxy="http://baturoa:myadel05@dcuproxy.dcu.ie:8080")
Sys.setenv(http_proxy="http://baturoa:myadel05@proxy.dcu.ie:8080")
install.packages("deldir")
install.packages(deldir_0.1-1.tar.gz, repos = NULL, type =``source'')
install.packages(deldir.tar.gz, repos = NULL, type =``source'')
library(deldir)
library(rJava)#
Sys.setenv(NOAWT=TRUE)#
library(tm)#
library(Snowball)#
library(gplots)#
library(austin)#
library(RWekajars)#
library(foreign)#
library(wordcloud)#
library(stringr)#
library(RColorBrewer)#
set.seed(100)
library(wordcloud)#
library(stringr)#
library(RColorBrewer)#
set.seed(100)
library(wordcloud)#
library(stringr)#
library(RColorBrewer)#
set.seed(100)
setwd('/ALEX/UN/Session1979')#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
files.path <- '/ALEX/UN/Session1979'#
graphs.path <- '/ALEX/UN/analysis'#
data.output <- '/ALEX/UN/analysis'
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus
text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="34"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="34"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=4 width=2)#
#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=4 width=2)#
#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=4 width=2)#
#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
files.path <- '/ALEX/UN/Session1979'#
graphs.path <- '/ALEX/UN/analysis'#
data.output <- '/ALEX/UN/analysis'
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=4 width=2)#
#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=4, width=2)#
#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=4, width=2)#
#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=6, width=3)#
#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=6, width=4)#
#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=8, width=5)#
#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=8, width=5)#
#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
	par(mar=c(5,3,2,2)+0.1)#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=8, width=5)#
#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
	par(mar=c(4,0,2,0)+0.1)#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=8, width=5)#
par(mar=c(4,0,2,0)+0.1)#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=8, width=5)#
par(mar=c(4,1,2,1)+0.1)#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.1)#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_ci.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
	plot(positions,ydummy,#
		, main = "UN 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,144),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
setwd("/ALEX/Dropbox/UN General Debate/Data/Session 16 - 1962")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 16 - 1962"#
graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus
text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]
inspect(text.corpus.format[1:2])
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="16"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="16"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1962.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1962.csv", row.names=docnames)
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1962.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1962"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.7),#
		ylim=c(1,80),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1962.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1962"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.7),#
		ylim=c(1,80),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)
getwd()
setwd("/ALEX/Dropbox/UN General Debate/Data/Session 16 - 1962")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 16 - 1962")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 16 - 1962"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 16 - 1962"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Analysis"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Analysis"
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus
text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="16"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="16"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta
write.table(positions, row.names=docnames, file="results1962.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta
write.table(standard, file="se1962.csv", row.names=docnames)
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1962.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1962"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.7),#
		ylim=c(1,80),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
ref.left <- seq(1,N,1)[countryname=="USSR" & year=="16"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="16"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta
out.file <- paste(graphs.path,"wordcloud_1962.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1962.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1962"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.7),#
		ylim=c(1,80),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()
out.file <- paste(graphs.path,"wordcloud_1962.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()
write.table(wcdata.reduced, file="wordfreq1962.csv")#
sink(file="results1962")#
results <- summary(wf)#
summary(wf)#
sink(NULL)
--------------------#
# 1977#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 32 - 1977")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 32 - 1977")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 32 - 1977"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 32 - 1977"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="32"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="32"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1977.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1977.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1977.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1977"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,141),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1977.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1977.csv")#
sink(file="results1977")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1978#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 33 - 1978")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 33 - 1978")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 33 - 1978"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 33 - 1978"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="33"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="33"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1978.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1978.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1978.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,141),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1978.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1978.csv")#
sink(file="results1978")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1979#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 34 - 1979")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 34 - 1979")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 34 - 1979"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 34 - 1979"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="34"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="34"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1979.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1979.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1979.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1979"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,145),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1979.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1979.csv")#
sink(file="results1979")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1980#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 35 - 1980")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 35 - 1980")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 35 - 1980"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 35 - 1980"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="35"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="35"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1980.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1980.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1980.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1980"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,151),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1980.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1980.csv")#
sink(file="results1980")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1982#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 37 - 1982")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 37 - 1982")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 37 - 1982"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 37 - 1982"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Analysis"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Analysis"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="37"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="37"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1982.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1982.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1982.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1982"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,142),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1982.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1982.csv")#
sink(file="results1982")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1983#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 38 - 1983")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 38 - 1983")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 38 - 1983"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 38 - 1983"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Analysis"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Analysis"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="38"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="38"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1983.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1983.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1983.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1983"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,151),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1983.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1983.csv")#
sink(file="results1983")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1984#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 39 - 1984")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 39 - 1984")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 39 - 1984"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 39 - 1984"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Analysis"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Analysis"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="39"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="39"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1984.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1984.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1984.pdf",sep="")#
pdf(file=out.file)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1984"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,150),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1984.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1984.csv")#
sink(file="results1984")#
results <- summary(wf)#
summary(wf)#
sink(NULL)
--------------------------------------------------------------------------------#
# UNGA WORDFISH#
# --------------------------------------------------------------------------------#
#
library(rJava)#
Sys.setenv(NOAWT=TRUE)#
library(tm)#
library(Snowball)#
library(gplots)#
library(austin)#
library(RWekajars)#
library(foreign)#
library(wordcloud)#
set.seed(100)#
# --------------------#
# 1962#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 16 - 1962")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 16 - 1962")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 16 - 1962"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 16 - 1962"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="16"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="16"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1962.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1962.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1962.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1962"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.7),#
		ylim=c(1,80),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1962.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1962.csv")#
sink(file="results1962")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1977#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 32 - 1977")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 32 - 1977")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 32 - 1977"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 32 - 1977"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="32"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="32"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1977.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1977.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1977.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1977"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,141),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1977.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1977.csv")#
sink(file="results1977")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1978#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 33 - 1978")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 33 - 1978")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 33 - 1978"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 33 - 1978"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="33"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="33"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1978.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1978.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1978.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1978"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,141),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1978.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1978.csv")#
sink(file="results1978")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1979#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 34 - 1979")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 34 - 1979")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 34 - 1979"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 34 - 1979"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="34"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="34"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1979.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1979.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1979.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1979"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,145),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1979.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1979.csv")#
sink(file="results1979")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1980#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 35 - 1980")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 35 - 1980")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 35 - 1980"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 35 - 1980"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="35"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="35"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1980.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1980.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1980.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1980"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,151),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1980.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1980.csv")#
sink(file="results1980")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1982#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 37 - 1982")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 37 - 1982")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 37 - 1982"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 37 - 1982"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="37"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="37"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1982.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1982.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1982.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1982"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,142),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1982.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1982.csv")#
sink(file="results1982")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1983#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 38 - 1983")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 38 - 1983")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 38 - 1983"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 38 - 1983"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="38"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="38"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1983.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1983.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1983.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1983"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,151),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1983.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1983.csv")#
sink(file="results1983")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1984#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 39 - 1984")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 39 - 1984")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 39 - 1984"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 39 - 1984"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="39"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="39"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1984.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1984.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1984.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1984"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,150),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1984.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1984.csv")#
sink(file="results1984")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1987#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 42 - 1987")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 42 - 1987")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 42 - 1987"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 42 - 1987"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="42"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="42"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1987.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1987.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1987.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1987"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,149),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1987.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1987.csv")#
sink(file="results1987")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1988#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 43 - 1988")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 43 - 1988")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 43 - 1988"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 43 - 1988"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="43"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="43"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1988.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1988.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1988.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1988"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,156),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1988.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1988.csv")#
sink(file="results1988")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1989#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 44 - 1989")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 44 - 1989")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 44 - 1989"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 44 - 1989"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="44"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="44"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1989.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1989.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1989.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1989"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,155),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1989.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1989.csv")#
sink(file="results1989")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1990#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 45 - 1990")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 45 - 1990")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 45 - 1990"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 45 - 1990"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="45"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="45"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1990.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1990.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1990.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1990"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,158),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1990.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1990.csv")#
sink(file="results1990")#
results <- summary(wf)#
summary(wf)#
sink(NULL)#
# --------------------#
# 1991#
# --------------------#
# setwd("/ALEX/Dropbox/UN General Debate/Data/Session 46 - 1991")#
setwd("/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 46 - 1991")#
#
# define directory that contains text files.#
# Note:#
# - files must have ending .txt#
# - directory cannot contain any other files#
# - file names will be used as document names#
#
#files.path <- "/ALEX/Dropbox/UN General Debate/Data/Session 46 - 1991"#
#graphs.path <- "/ALEX/Dropbox/UN General Debate/Analysis"#
#data.output <- "/ALEX/Dropbox/UN General Debate/Analysis"#
files.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/Data/Session 46 - 1991"#
graphs.path <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
data.output <- "/Users/alexanderbaturo/Dropbox/UN General Debate/"#
#
# --------------------------------------------------------------------------------#
# TEXT CORPUS PROCESSING#
# --------------------------------------------------------------------------------#
# generate text corpus#
#
text.file.directory <- files.path#
textcorpus <- Corpus(DirSource(text.file.directory), readerControl=list(reader=readPlain,language="english", load=T))#
# extract document names for tracking purposes#
docnames <- list.files(text.file.directory)#
for (i in 1:length(textcorpus)) {#
  docnames[i] <- strsplit(docnames[i],".txt")[[1]]#
  }#
text.corpus.format<-textcorpus#
# --------------------------------------------------------------------------------#
# # # text cleaning#
#
	# remove numbers#
	text.corpus.format <- tm_map(text.corpus.format, removeNumbers)#
			# remove punctuation#
text.corpus.format <-tm_map(text.corpus.format,removePunctuation)#
		    text.corpus.format  <- tm_map(text.corpus.format, removeWords,   c(stopwords("english"),"The", "the"))#
# cleaning texts #
#	text.corpus.format <- tm_map(text.corpus.format, tolower)       #
     		# remove all breaks #
#	text.corpus.format <-tm_map(text.corpus.format,PlainTextDocument)#
	# strip whitespace#
	#  text.corpus.format<-tm_map(text.corpus.format,stripWhitespace)  #
	# stem words#
	#  text.corpus.format <-tm_map(text.corpus.format,stemDocument,language="english")#
# --------------------------------------------------------------------------------#
wordfreqmatrix <- DocumentTermMatrix(text.corpus.format)#
wcdata <- as.matrix(wordfreqmatrix)#
rownames(wcdata) <- docnames#
wcdata <- t(wcdata)#
# AB: gives number of items to replace is not a multiple of replacement length#
N <- dim(wcdata)[2]#
#
# generate names from colnames#
full.names <- docnames#
countryname <- rep(NA,N)#
year <- rep(NA,N)#
#
for (i in 1:N) {#
  countryname[i] <- strsplit(full.names[i],"_")[[1]][1]#
  year[i] <- strsplit(full.names[i],"_")[[1]][2]#
}#
#
# delete words that appear in less than 3 documents#
dummy.matrix <- wcdata#
dummy.matrix[dummy.matrix>0] <- 1#
wcdata.reduced <- wcdata[rowSums(dummy.matrix) >= 3, ]#
#
# check for errors#
# inspect(text.corpus.format[1:2])#
#
# --------------------------------------------------------------------------------#
# ESTIMATION#
# --------------------------------------------------------------------------------#
#
ref.left <- seq(1,N,1)[countryname=="CUB" & year=="46"]#
ref.right <- seq(1,N,1)[countryname=="USA" & year=="46"]#
#
m <- wfm(wcdata.reduced, word.margin=1)#
wf <- wordfish(m, dir=c(ref.left,ref.right), verbose=TRUE)#
summary(wf)#
positions <- wf$theta#
# write.csv(t(as.matrix(positions)), file="results.csv")#
write.table(positions, row.names=docnames, file="results1991.csv")#
standard <- wf$se.theta#
se.lb <- wf$theta -2*wf$se.theta#
se.ub <- wf$theta +2*wf$se.theta#
#
write.table(standard, file="se1991.csv", row.names=docnames)#
## # -------------------------------#
## # PLOT ESTIMATED POSITIONS #
## # -------------------------------#
#
# y-axis variable: rank of positins by year#
ydummy <- (-1)*(rank(positions)-N)#
# label names#
lab.name <- paste(countryname,year)#
#
out.file <- paste(graphs.path,"positions_1991.pdf",sep="")#
pdf(file=out.file, height=8, width=4)#
par(mar=c(4,1,2,1)+0.2)#
#
	plot(positions,ydummy,#
		, main = "UNGA 1991"#
		, xlab="Estimated Positions", ylab = " ", yaxt = "n"#
		, cex=0.2#
		, pch = 1#
		, col = "orange"#
		, xlim = c(-2.5, 2.5),#
		ylim=c(1,151),#
		)	#
		for (i in seq(1,length(ydummy))){#
			lines(c(se.lb[i], se.ub[i]), c(ydummy[i],ydummy[i])#
			, col = "orange"#
			, lwd = 1#
		)#
	}#
text(x=positions,ydummy, labels=countryname, cex=0.25, pos=4)#
abline(v=0,col=3,lty=5)#
dev.off()#
## # -------------------------------#
## # PLOT WORD CLOUD #
## # -------------------------------#
#
out.file <- paste(graphs.path,"wordcloud_1991.pdf",sep="")#
pdf(file=out.file)#
wordcloud(text.corpus.format, scale=c(5,0.5), max.words=150,  random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))#
dev.off()#
## # save word frequency matrix in csv and results as txt#
#
write.table(wcdata.reduced, file="wordfreq1991.csv")#
sink(file="results1991")#
results <- summary(wf)#
summary(wf)#
sink(NULL)
